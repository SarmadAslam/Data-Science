{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7RkAXxzbeRd1YRUw0Xuw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarmadAslam/Data-Science/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hRyKrEDkj7L7"
      },
      "outputs": [],
      "source": [
        "# !pip install pdfplumber pytesseract pdf2image sentence-transformers faiss-cpu transformers fuzzywuzzy python-Levenshtein streamlit pyngrok requests pillow torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 🛠️ 1. System-Level Dependencies (for Camelot, Tesseract, Tabula)\n",
        "# !apt-get update\n",
        "# !apt-get install -y ghostscript python3-tk tesseract-ocr openjdk-11-jre\n",
        "\n",
        "# # 🧠 2. Download spaCy language model\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# # 📦 3. Python Dependencies\n",
        "# !pip install \\\n",
        "#     google-generativeai \\\n",
        "#     pandas \\\n",
        "#     spacy \\\n",
        "#     camelot-py[cv] \\\n",
        "#     tabula-py \\\n",
        "#     pdfminer.six \\\n",
        "#     PyMuPDF \\\n",
        "#     scikit-learn \\\n",
        "#     phonenumbers \\\n",
        "#     email-validator \\\n",
        "#     backoff\n"
      ],
      "metadata": {
        "id": "r5Plm2FaxpcT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y tesseract-ocr\n",
        "# !apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "EW7qzaKikhXM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 30mDYr0DqRNWeehRC3sQ5qDqnJg_2EoFLvkY2GTo82ydKvRCA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZD_eEZPklUs",
        "outputId": "30af86b1-efac-49e9-9180-69602b8c9aee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# script_content = '''\n",
        "# import pdfplumber\n",
        "# import pytesseract\n",
        "# from PIL import Image\n",
        "# import pdf2image\n",
        "# import re\n",
        "# import numpy as np\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# import faiss\n",
        "# import streamlit as st\n",
        "# import requests\n",
        "# import json\n",
        "# import os\n",
        "# import logging\n",
        "# import traceback\n",
        "# import google.generativeai as genai\n",
        "# from typing import List, Dict, Any, Optional, Tuple\n",
        "# import pandas as pd\n",
        "# from dataclasses import dataclass, field\n",
        "# import spacy\n",
        "# from fuzzywuzzy import fuzz\n",
        "# import time\n",
        "# from collections import defaultdict\n",
        "# import hashlib\n",
        "# from datetime import datetime\n",
        "# import camelot\n",
        "# import tabula\n",
        "# from pdfminer.high_level import extract_text\n",
        "# from pdfminer.layout import LAParams\n",
        "# import fitz  # PyMuPDF\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import phonenumbers\n",
        "# import email_validator\n",
        "# from urllib.parse import urlparse\n",
        "# import concurrent.futures\n",
        "# from functools import lru_cache\n",
        "# import backoff\n",
        "\n",
        "# # Configure logging\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "# )\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "# # API Configuration\n",
        "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "# DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"sk-640564380f8645a0aadb6168fc70a96a\")\n",
        "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "# # Configure Gemini\n",
        "# if GEMINI_API_KEY:\n",
        "#     genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# @dataclass\n",
        "# class Contact:\n",
        "#     \"\"\"Enhanced contact information with metadata\"\"\"\n",
        "#     name: str = \"\"\n",
        "#     business_name: str = \"\"\n",
        "#     email: str = \"\"\n",
        "#     phone: str = \"\"\n",
        "#     address: str = \"\"\n",
        "#     website: str = \"\"\n",
        "#     title: str = \"\"\n",
        "#     department: str = \"\"\n",
        "#     linkedin: str = \"\"\n",
        "#     fax: str = \"\"\n",
        "#     mobile: str = \"\"\n",
        "#     confidence_score: float = 0.0\n",
        "#     source_context: str = \"\"\n",
        "#     extraction_method: str = \"\"\n",
        "#     page_number: int = 0\n",
        "#     position_in_doc: Tuple[float, float] = (0.0, 0.0)\n",
        "#     metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "#     def __hash__(self):\n",
        "#         return hash((self.email, self.phone, self.name))\n",
        "\n",
        "#     def to_dict(self):\n",
        "#         return {k: v for k, v in self.__dict__.items() if v}\n",
        "\n",
        "# class DocumentAnalyzer:\n",
        "#     \"\"\"Analyzes PDF structure and content type\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.patterns = {\n",
        "#             'table': r'(?i)(name|email|phone|contact|address|website|fax|mobile)',\n",
        "#             'business_card': r'(?i)(tel|mob|email|www\\.|@)',\n",
        "#             'directory': r'(?i)(member|directory|list|roster|association)',\n",
        "#             'resume': r'(?i)(experience|education|skills|objective)',\n",
        "#             'form': r'(?i)(form|application|registration)',\n",
        "#         }\n",
        "\n",
        "#     def analyze_document(self, text: str, pages: List[Any]) -> Dict[str, Any]:\n",
        "#         \"\"\"Analyze document structure and type\"\"\"\n",
        "#         analysis = {\n",
        "#             'type': 'unknown',\n",
        "#             'has_tables': False,\n",
        "#             'is_scanned': False,\n",
        "#             'language': 'en',\n",
        "#             'contact_density': 0,\n",
        "#             'structure_type': 'unstructured',\n",
        "#             'recommended_methods': []\n",
        "#         }\n",
        "\n",
        "#         # Check if scanned\n",
        "#         if not text.strip() and pages:\n",
        "#             analysis['is_scanned'] = True\n",
        "\n",
        "#         # Detect document type\n",
        "#         text_lower = text.lower()\n",
        "#         for doc_type, pattern in self.patterns.items():\n",
        "#             if len(re.findall(pattern, text_lower)) > 3:\n",
        "#                 analysis['type'] = doc_type\n",
        "#                 break\n",
        "\n",
        "#         # Calculate contact density\n",
        "#         contact_indicators = len(re.findall(r'(?i)(email|phone|tel|mobile|@)', text))\n",
        "#         analysis['contact_density'] = contact_indicators / max(len(text.split()), 1)\n",
        "\n",
        "#         # Recommend extraction methods\n",
        "#         if analysis['type'] == 'table' or 'table' in text_lower:\n",
        "#             analysis['recommended_methods'] = ['table_extraction', 'structured_parsing']\n",
        "#             analysis['structure_type'] = 'tabular'\n",
        "#         elif analysis['is_scanned']:\n",
        "#             analysis['recommended_methods'] = ['ocr_enhanced', 'layout_analysis']\n",
        "#         elif analysis['contact_density'] > 0.01:\n",
        "#             analysis['recommended_methods'] = ['pattern_matching', 'ai_extraction']\n",
        "#             analysis['structure_type'] = 'semi_structured'\n",
        "#         else:\n",
        "#             analysis['recommended_methods'] = ['ai_extraction', 'nlp_parsing']\n",
        "\n",
        "#         return analysis\n",
        "\n",
        "# class IntelligentPDFExtractor:\n",
        "#     \"\"\"Multi-method PDF extraction with intelligent fallbacks\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.analyzer = DocumentAnalyzer()\n",
        "\n",
        "#     def extract_with_pdfplumber(self, pdf_path: str) -> Tuple[str, List[Dict]]:\n",
        "#         \"\"\"Extract using pdfplumber with enhanced parsing\"\"\"\n",
        "#         text = \"\"\n",
        "#         tables = []\n",
        "\n",
        "#         try:\n",
        "#             with pdfplumber.open(pdf_path) as pdf:\n",
        "#                 for page_num, page in enumerate(pdf.pages):\n",
        "#                     # Extract text\n",
        "#                     page_text = page.extract_text() or \"\"\n",
        "#                     text += f\"\\n--- PAGE {page_num + 1} ---\\n{page_text}\\n\"\n",
        "\n",
        "#                     # Extract tables\n",
        "#                     page_tables = page.extract_tables()\n",
        "#                     if page_tables:\n",
        "#                         for table in page_tables:\n",
        "#                             if table:\n",
        "#                                 tables.append({\n",
        "#                                     'page': page_num + 1,\n",
        "#                                     'data': table\n",
        "#                                 })\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"PDFPlumber extraction failed: {e}\")\n",
        "\n",
        "#         return text, tables\n",
        "\n",
        "#     def extract_with_camelot(self, pdf_path: str) -> List[pd.DataFrame]:\n",
        "#         \"\"\"Extract tables using Camelot\"\"\"\n",
        "#         try:\n",
        "#             tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')\n",
        "#             if not tables:\n",
        "#                 tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
        "#             return [table.df for table in tables]\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Camelot extraction failed: {e}\")\n",
        "#             return []\n",
        "\n",
        "#     def extract_with_tabula(self, pdf_path: str) -> List[pd.DataFrame]:\n",
        "#         \"\"\"Extract tables using Tabula\"\"\"\n",
        "#         try:\n",
        "#             return tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Tabula extraction failed: {e}\")\n",
        "#             return []\n",
        "\n",
        "#     def extract_with_pdfminer(self, pdf_path: str) -> str:\n",
        "#         \"\"\"Extract with PDFMiner for better layout preservation\"\"\"\n",
        "#         try:\n",
        "#             laparams = LAParams(\n",
        "#                 line_overlap=0.5,\n",
        "#                 char_margin=2.0,\n",
        "#                 word_margin=0.1,\n",
        "#                 boxes_flow=0.5\n",
        "#             )\n",
        "#             return extract_text(pdf_path, laparams=laparams)\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"PDFMiner extraction failed: {e}\")\n",
        "#             return \"\"\n",
        "\n",
        "#     def extract_with_pymupdf(self, pdf_path: str) -> Tuple[str, List[Dict]]:\n",
        "#         \"\"\"Extract using PyMuPDF with layout analysis\"\"\"\n",
        "#         text = \"\"\n",
        "#         metadata = []\n",
        "\n",
        "#         try:\n",
        "#             doc = fitz.open(pdf_path)\n",
        "#             for page_num, page in enumerate(doc):\n",
        "#                 # Extract text with position\n",
        "#                 page_text = page.get_text()\n",
        "#                 text += f\"\\n--- PAGE {page_num + 1} ---\\n{page_text}\\n\"\n",
        "\n",
        "#                 # Extract text blocks with positions\n",
        "#                 blocks = page.get_text(\"dict\")\n",
        "#                 for block in blocks[\"blocks\"]:\n",
        "#                     if \"lines\" in block:\n",
        "#                         for line in block[\"lines\"]:\n",
        "#                             for span in line[\"spans\"]:\n",
        "#                                 metadata.append({\n",
        "#                                     'page': page_num + 1,\n",
        "#                                     'text': span['text'],\n",
        "#                                     'bbox': span['bbox'],\n",
        "#                                     'font': span.get('font', ''),\n",
        "#                                     'size': span.get('size', 0)\n",
        "#                                 })\n",
        "#             doc.close()\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"PyMuPDF extraction failed: {e}\")\n",
        "\n",
        "#         return text, metadata\n",
        "\n",
        "#     def enhanced_ocr_extraction(self, pdf_path: str) -> str:\n",
        "#         \"\"\"Enhanced OCR with preprocessing\"\"\"\n",
        "#         text = \"\"\n",
        "\n",
        "#         try:\n",
        "#             images = pdf2image.convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "#             for i, img in enumerate(images):\n",
        "#                 # Preprocess image\n",
        "#                 img = img.convert('L')  # Grayscale\n",
        "#                 img_array = np.array(img)\n",
        "\n",
        "#                 # Adaptive thresholding\n",
        "#                 from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
        "#                 img_array = adaptiveThreshold(\n",
        "#                     img_array, 255,\n",
        "#                     ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "#                     THRESH_BINARY, 11, 2\n",
        "#                 )\n",
        "\n",
        "#                 # OCR with multiple configurations\n",
        "#                 configs = [\n",
        "#                     '--psm 6',  # Uniform block of text\n",
        "#                     '--psm 4',  # Single column\n",
        "#                     '--psm 3',  # Fully automatic\n",
        "#                 ]\n",
        "\n",
        "#                 best_text = \"\"\n",
        "#                 max_conf = 0\n",
        "\n",
        "#                 for config in configs:\n",
        "#                     try:\n",
        "#                         data = pytesseract.image_to_data(\n",
        "#                             img_array,\n",
        "#                             config=config,\n",
        "#                             output_type=pytesseract.Output.DICT\n",
        "#                         )\n",
        "\n",
        "#                         conf = np.mean([int(c) for c in data['conf'] if int(c) > 0])\n",
        "#                         page_text = \" \".join([\n",
        "#                             data['text'][i]\n",
        "#                             for i in range(len(data['text']))\n",
        "#                             if int(data['conf'][i]) > 30\n",
        "#                         ])\n",
        "\n",
        "#                         if conf > max_conf:\n",
        "#                             max_conf = conf\n",
        "#                             best_text = page_text\n",
        "#                     except:\n",
        "#                         continue\n",
        "\n",
        "#                 text += f\"\\n--- PAGE {i + 1} ---\\n{best_text}\\n\"\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Enhanced OCR failed: {e}\")\n",
        "\n",
        "#         return text\n",
        "\n",
        "#     def extract_all_methods(self, pdf_path: str) -> Dict[str, Any]:\n",
        "#         \"\"\"Extract using all methods and combine results\"\"\"\n",
        "#         results = {\n",
        "#             'text': '',\n",
        "#             'tables': [],\n",
        "#             'metadata': [],\n",
        "#             'extraction_methods': []\n",
        "#         }\n",
        "\n",
        "#         # Analyze document first\n",
        "#         with pdfplumber.open(pdf_path) as pdf:\n",
        "#             sample_text = \"\"\n",
        "#             for i, page in enumerate(pdf.pages[:3]):  # Sample first 3 pages\n",
        "#                 sample_text += (page.extract_text() or \"\")\n",
        "\n",
        "#             analysis = self.analyzer.analyze_document(sample_text, pdf.pages)\n",
        "\n",
        "#         # Apply recommended methods\n",
        "#         if 'table_extraction' in analysis['recommended_methods']:\n",
        "#             # Try table extraction methods\n",
        "#             camelot_tables = self.extract_with_camelot(pdf_path)\n",
        "#             if camelot_tables:\n",
        "#                 results['tables'].extend([{\n",
        "#                     'method': 'camelot',\n",
        "#                     'data': table\n",
        "#                 } for table in camelot_tables])\n",
        "#                 results['extraction_methods'].append('camelot')\n",
        "\n",
        "#             tabula_tables = self.extract_with_tabula(pdf_path)\n",
        "#             if tabula_tables:\n",
        "#                 results['tables'].extend([{\n",
        "#                     'method': 'tabula',\n",
        "#                     'data': table\n",
        "#                 } for table in tabula_tables])\n",
        "#                 results['extraction_methods'].append('tabula')\n",
        "\n",
        "#         # Extract text with multiple methods\n",
        "#         text1, plumber_tables = self.extract_with_pdfplumber(pdf_path)\n",
        "#         if text1:\n",
        "#             results['text'] = text1\n",
        "#             results['extraction_methods'].append('pdfplumber')\n",
        "#             if plumber_tables:\n",
        "#                 results['tables'].extend([{\n",
        "#                     'method': 'pdfplumber',\n",
        "#                     'data': table\n",
        "#                 } for table in plumber_tables])\n",
        "\n",
        "#         # Try PDFMiner for better layout\n",
        "#         if not results['text'] or analysis['structure_type'] == 'semi_structured':\n",
        "#             text2 = self.extract_with_pdfminer(pdf_path)\n",
        "#             if text2 and len(text2) > len(results['text']):\n",
        "#                 results['text'] = text2\n",
        "#                 results['extraction_methods'].append('pdfminer')\n",
        "\n",
        "#         # Try PyMuPDF for metadata\n",
        "#         text3, metadata = self.extract_with_pymupdf(pdf_path)\n",
        "#         if metadata:\n",
        "#             results['metadata'] = metadata\n",
        "#             results['extraction_methods'].append('pymupdf')\n",
        "#             if not results['text']:\n",
        "#                 results['text'] = text3\n",
        "\n",
        "#         # OCR as last resort or if scanned\n",
        "#         if not results['text'].strip() or analysis['is_scanned']:\n",
        "#             ocr_text = self.enhanced_ocr_extraction(pdf_path)\n",
        "#             if ocr_text:\n",
        "#                 results['text'] = ocr_text\n",
        "#                 results['extraction_methods'].append('enhanced_ocr')\n",
        "\n",
        "#         results['analysis'] = analysis\n",
        "#         return results\n",
        "\n",
        "# class IntelligentTextChunker:\n",
        "#     \"\"\"Context-aware text chunking that preserves contact boundaries\"\"\"\n",
        "\n",
        "#     def __init__(self, nlp_model=None):\n",
        "#         self.nlp = nlp_model\n",
        "#         self.contact_patterns = [\n",
        "#             r'(?i)(?:name|contact|person).*?(?:email|phone|tel|mobile|address)',\n",
        "#             r'(?i)(?:mr|mrs|ms|dr|prof)\\.?\\s+[A-Z][a-z]+.*?(?:@|\\d{3,})',\n",
        "#             r'[A-Z][a-z]+\\s+[A-Z][a-z]+.*?(?:\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b)',\n",
        "#         ]\n",
        "\n",
        "#     def identify_contact_boundaries(self, text: str) -> List[Tuple[int, int]]:\n",
        "#         \"\"\"Identify potential contact information boundaries\"\"\"\n",
        "#         boundaries = []\n",
        "\n",
        "#         # Find pattern-based boundaries\n",
        "#         for pattern in self.contact_patterns:\n",
        "#             for match in re.finditer(pattern, text, re.DOTALL):\n",
        "#                 boundaries.append((match.start(), match.end()))\n",
        "\n",
        "#         # Merge overlapping boundaries\n",
        "#         boundaries.sort()\n",
        "#         merged = []\n",
        "#         for start, end in boundaries:\n",
        "#             if merged and start <= merged[-1][1] + 50:  # Allow 50 char gap\n",
        "#                 merged[-1] = (merged[-1][0], max(merged[-1][1], end))\n",
        "#             else:\n",
        "#                 merged.append((start, end))\n",
        "\n",
        "#         return merged\n",
        "\n",
        "#     def chunk_by_structure(self, text: str, tables: List[Dict]) -> List[Dict[str, Any]]:\n",
        "#         \"\"\"Create chunks based on document structure\"\"\"\n",
        "#         chunks = []\n",
        "\n",
        "#         # Handle tables separately\n",
        "#         for table in tables:\n",
        "#             if isinstance(table.get('data'), pd.DataFrame):\n",
        "#                 chunks.append({\n",
        "#                     'type': 'table',\n",
        "#                     'content': table['data'].to_string(),\n",
        "#                     'data': table['data'],\n",
        "#                     'method': table.get('method', 'unknown')\n",
        "#                 })\n",
        "#             elif isinstance(table.get('data'), list):\n",
        "#                 chunks.append({\n",
        "#                     'type': 'table',\n",
        "#                     'content': '\\n'.join(['\\t'.join(map(str, row)) for row in table['data'] if row]),\n",
        "#                     'data': table['data'],\n",
        "#                     'method': table.get('method', 'unknown')\n",
        "#                 })\n",
        "\n",
        "#         # Handle text with smart chunking\n",
        "#         if text:\n",
        "#             # Split by pages first\n",
        "#             pages = re.split(r'--- PAGE \\d+ ---', text)\n",
        "\n",
        "#             for page_num, page_text in enumerate(pages):\n",
        "#                 if not page_text.strip():\n",
        "#                     continue\n",
        "\n",
        "#                 # Find contact boundaries in this page\n",
        "#                 boundaries = self.identify_contact_boundaries(page_text)\n",
        "\n",
        "#                 if boundaries:\n",
        "#                     # Chunk based on boundaries\n",
        "#                     last_end = 0\n",
        "#                     for start, end in boundaries:\n",
        "#                         # Include context before\n",
        "#                         context_start = max(0, start - 100)\n",
        "#                         context_end = min(len(page_text), end + 100)\n",
        "\n",
        "#                         chunk_text = page_text[context_start:context_end]\n",
        "#                         chunks.append({\n",
        "#                             'type': 'contact_region',\n",
        "#                             'content': chunk_text,\n",
        "#                             'page': page_num + 1,\n",
        "#                             'confidence': 0.8\n",
        "#                         })\n",
        "#                         last_end = end\n",
        "#                 else:\n",
        "#                     # Smart paragraph chunking\n",
        "#                     paragraphs = re.split(r'\\n\\s*\\n', page_text)\n",
        "#                     current_chunk = \"\"\n",
        "\n",
        "#                     for para in paragraphs:\n",
        "#                         if len(current_chunk) + len(para) > 1500:\n",
        "#                             if current_chunk:\n",
        "#                                 chunks.append({\n",
        "#                                     'type': 'text',\n",
        "#                                     'content': current_chunk,\n",
        "#                                     'page': page_num + 1\n",
        "#                                 })\n",
        "#                             current_chunk = para\n",
        "#                         else:\n",
        "#                             current_chunk += \"\\n\\n\" + para\n",
        "\n",
        "#                     if current_chunk:\n",
        "#                         chunks.append({\n",
        "#                             'type': 'text',\n",
        "#                             'content': current_chunk,\n",
        "#                             'page': page_num + 1\n",
        "#                         })\n",
        "\n",
        "#         return chunks\n",
        "\n",
        "#     def semantic_chunking(self, text: str, max_chunk_size: int = 1000) -> List[str]:\n",
        "#         \"\"\"Chunk based on semantic boundaries using NLP\"\"\"\n",
        "#         if not self.nlp:\n",
        "#             return self.simple_chunking(text, max_chunk_size)\n",
        "\n",
        "#         try:\n",
        "#             doc = self.nlp(text[:1000000])  # Limit for performance\n",
        "#             chunks = []\n",
        "#             current_chunk = []\n",
        "#             current_size = 0\n",
        "\n",
        "#             for sent in doc.sents:\n",
        "#                 sent_text = sent.text.strip()\n",
        "#                 sent_size = len(sent_text)\n",
        "\n",
        "#                 # Check if this sentence likely contains contact info\n",
        "#                 has_contact = any(\n",
        "#                     ent.label_ in ['PERSON', 'ORG', 'EMAIL', 'PHONE']\n",
        "#                     for ent in sent.ents\n",
        "#                 )\n",
        "\n",
        "#                 if current_size + sent_size > max_chunk_size and current_chunk:\n",
        "#                     # Don't break if we're in the middle of contact info\n",
        "#                     if not has_contact or current_size > max_chunk_size * 1.5:\n",
        "#                         chunks.append(' '.join(current_chunk))\n",
        "#                         current_chunk = [sent_text]\n",
        "#                         current_size = sent_size\n",
        "#                     else:\n",
        "#                         current_chunk.append(sent_text)\n",
        "#                         current_size += sent_size\n",
        "#                 else:\n",
        "#                     current_chunk.append(sent_text)\n",
        "#                     current_size += sent_size\n",
        "\n",
        "#             if current_chunk:\n",
        "#                 chunks.append(' '.join(current_chunk))\n",
        "\n",
        "#             return chunks\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Semantic chunking failed: {e}\")\n",
        "#             return self.simple_chunking(text, max_chunk_size)\n",
        "\n",
        "#     def simple_chunking(self, text: str, chunk_size: int = 1000) -> List[str]:\n",
        "#         \"\"\"Fallback simple chunking with overlap\"\"\"\n",
        "#         chunks = []\n",
        "#         sentences = re.split(r'[.!?\\n]+', text)\n",
        "#         current_chunk = \"\"\n",
        "\n",
        "#         for sentence in sentences:\n",
        "#             if len(current_chunk) + len(sentence) > chunk_size:\n",
        "#                 if current_chunk:\n",
        "#                     chunks.append(current_chunk)\n",
        "#                 current_chunk = sentence\n",
        "#             else:\n",
        "#                 current_chunk += \" \" + sentence\n",
        "\n",
        "#         if current_chunk:\n",
        "#             chunks.append(current_chunk)\n",
        "\n",
        "#         return chunks\n",
        "\n",
        "# class AdaptivePromptEngine:\n",
        "#     \"\"\"Generates context-aware prompts based on document analysis\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.prompt_templates = {\n",
        "#             'table': \"\"\"\n",
        "# You are analyzing a TABULAR document with structured contact information.\n",
        "\n",
        "# EXTRACTION RULES:\n",
        "# 1. Each row likely represents one contact\n",
        "# 2. Column headers indicate data types (Name, Email, Phone, etc.)\n",
        "# 3. Maintain row integrity - don't mix data between rows\n",
        "# 4. Handle multi-line cells and merged cells correctly\n",
        "# 5. Extract ALL available fields including: Name, Title, Company, Email, Phone, Mobile, Fax, Address, Website\n",
        "\n",
        "# TABLE DATA:\n",
        "# {content}\n",
        "\n",
        "# Return ONLY a JSON array with complete contact objects. Include confidence scores based on data completeness.\n",
        "# \"\"\",\n",
        "\n",
        "#             'directory': \"\"\"\n",
        "# You are extracting from a BUSINESS DIRECTORY or MEMBERSHIP LIST.\n",
        "\n",
        "# PATTERN RECOGNITION:\n",
        "# 1. Entries may follow patterns like: \"Company Name - Contact Person - Phone/Email\"\n",
        "# 2. Look for section headers, member numbers, or categories\n",
        "# 3. Addresses often span multiple lines\n",
        "# 4. Some entries may have multiple contacts per organization\n",
        "# 5. Preserve hierarchical relationships (company -> employees)\n",
        "\n",
        "# DIRECTORY CONTENT:\n",
        "# {content}\n",
        "\n",
        "# Extract all contacts maintaining their organizational relationships. Return JSON array with nested structures if needed.\n",
        "# \"\"\",\n",
        "\n",
        "#             'business_card': \"\"\"\n",
        "# You are extracting from BUSINESS CARD style layouts.\n",
        "\n",
        "# EXTRACTION FOCUS:\n",
        "# 1. Primary contact is usually the card owner\n",
        "# 2. Look for job titles near names\n",
        "# 3. Company logos/names are typically prominent\n",
        "# 4. Multiple phone numbers may be listed (mobile, office, fax)\n",
        "# 5. Social media handles and websites are common\n",
        "\n",
        "# CONTENT:\n",
        "# {content}\n",
        "\n",
        "# Extract complete contact information with high confidence for clearly identified fields.\n",
        "# \"\"\",\n",
        "\n",
        "#             'unstructured': \"\"\"\n",
        "# You are an expert contact extraction specialist analyzing an unstructured document.\n",
        "\n",
        "# ADVANCED EXTRACTION TECHNIQUES:\n",
        "# 1. Use contextual clues to associate information\n",
        "# 2. Names often appear before or after titles\n",
        "# 3. Email patterns: [name]@[domain].[tld]\n",
        "# 4. Phone patterns vary by country - extract all formats\n",
        "# 5. Websites may not include http://\n",
        "# 6. LinkedIn URLs often contain profile names\n",
        "# 7. Addresses may be incomplete - extract what's available\n",
        "\n",
        "# CRITICAL RULES:\n",
        "# - Each contact MUST have at least email OR phone to be valid\n",
        "# - Associate information based on proximity and context\n",
        "# - Don't create contacts from partial information\n",
        "# - Include confidence scores (0.0-1.0) based on:\n",
        "#   * Information completeness\n",
        "#   * Clear associations\n",
        "#   * Data quality\n",
        "\n",
        "# DOCUMENT CONTENT:\n",
        "# {content}\n",
        "\n",
        "# METADATA:\n",
        "# {metadata}\n",
        "\n",
        "# Return a JSON array of contact objects. Be conservative - only extract contacts you're confident about.\n",
        "# \"\"\",\n",
        "\n",
        "#             'mixed': \"\"\"\n",
        "# You are analyzing a document with MIXED FORMATS containing various contact information.\n",
        "\n",
        "# MULTI-FORMAT HANDLING:\n",
        "# 1. Document may contain tables, paragraphs, lists, and cards\n",
        "# 2. Use format-appropriate extraction for each section\n",
        "# 3. Cross-reference information when the same contact appears multiple times\n",
        "# 4. Handle variations in name spelling and formatting\n",
        "# 5. Merge duplicate entries intelligently\n",
        "\n",
        "# SECTION: {section_type}\n",
        "# {content}\n",
        "\n",
        "# Extract all valid contacts from this section. Consider the section type when applying extraction rules.\n",
        "# \"\"\"\n",
        "#         }\n",
        "\n",
        "#     def generate_prompt(self, chunk: Dict[str, Any], doc_analysis: Dict[str, Any]) -> str:\n",
        "#         \"\"\"Generate optimal prompt based on content analysis\"\"\"\n",
        "\n",
        "#         # Select base template\n",
        "#         doc_type = doc_analysis.get('type', 'unstructured')\n",
        "#         chunk_type = chunk.get('type', 'text')\n",
        "\n",
        "#         if chunk_type == 'table':\n",
        "#             template = self.prompt_templates['table']\n",
        "#         elif doc_type in self.prompt_templates:\n",
        "#             template = self.prompt_templates[doc_type]\n",
        "#         else:\n",
        "#             template = self.prompt_templates['unstructured']\n",
        "\n",
        "#         # Enhance prompt with specific instructions\n",
        "#         content = chunk.get('content', '')\n",
        "#         metadata = {\n",
        "#             'chunk_type': chunk_type,\n",
        "#             'page': chunk.get('page', 'unknown'),\n",
        "#             'extraction_method': chunk.get('method', 'unknown'),\n",
        "#             'document_type': doc_type,\n",
        "#             'confidence_hint': chunk.get('confidence', 0.5)\n",
        "#         }\n",
        "\n",
        "#         # Add dynamic instructions based on content analysis\n",
        "#         if '@' in content and '.com' in content:\n",
        "#             extra_instructions = \"\\nPay special attention to email addresses and their associated names.\"\n",
        "#         elif re.search(r'\\+?\\d{10,}', content):\n",
        "#             extra_instructions = \"\\nMultiple phone formats detected. Extract all variations.\"\n",
        "#         elif 'ltd' in content.lower() or 'inc' in content.lower() or 'corp' in content.lower():\n",
        "#             extra_instructions = \"\\nCorporate entities detected. Distinguish between company and person names.\"\n",
        "#         else:\n",
        "#             extra_instructions = \"\"\n",
        "\n",
        "#         # Fill template\n",
        "#         prompt = template.format(\n",
        "#             content=content,\n",
        "#             metadata=json.dumps(metadata, indent=2),\n",
        "#             section_type=chunk_type\n",
        "#         )\n",
        "\n",
        "#         return prompt + extra_instructions\n",
        "\n",
        "# class SmartAPIManager:\n",
        "#     \"\"\"Manages multiple APIs with intelligent fallback and rate limiting\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.api_configs = {\n",
        "#             'gemini': {\n",
        "#                 'enabled': bool(GEMINI_API_KEY),\n",
        "#                 'rate_limit': 60,  # requests per minute\n",
        "#                 'timeout': 30,\n",
        "#                 'priority': 1\n",
        "#             },\n",
        "#             'deepseek': {\n",
        "#                 'enabled': bool(DEEPSEEK_API_KEY),\n",
        "#                 'rate_limit': 30,\n",
        "#                 'timeout': 30,\n",
        "#                 'priority': 2\n",
        "#             },\n",
        "#             'openai': {\n",
        "#                 'enabled': bool(OPENAI_API_KEY),\n",
        "#                 'rate_limit': 60,\n",
        "#                 'timeout': 30,\n",
        "#                 'priority': 3\n",
        "#             }\n",
        "#         }\n",
        "\n",
        "#         self.request_history = defaultdict(list)\n",
        "#         self.prompt_engine = AdaptivePromptEngine()\n",
        "\n",
        "#     def _check_rate_limit(self, api_name: str) -> bool:\n",
        "#         \"\"\"Check if API rate limit allows request\"\"\"\n",
        "#         config = self.api_configs[api_name]\n",
        "#         now = time.time()\n",
        "\n",
        "#         # Clean old requests\n",
        "#         self.request_history[api_name] = [\n",
        "#             t for t in self.request_history[api_name]\n",
        "#             if now - t < 60\n",
        "#         ]\n",
        "\n",
        "#         return len(self.request_history[api_name]) < config['rate_limit']\n",
        "\n",
        "#     @backoff.on_exception(\n",
        "#         backoff.expo,\n",
        "#         (requests.exceptions.RequestException, Exception),\n",
        "#         max_tries=3\n",
        "#     )\n",
        "#     def _call_gemini(self, prompt: str) -> Optional[List[Dict]]:\n",
        "#         \"\"\"Call Gemini API with retry logic\"\"\"\n",
        "#         if not self._check_rate_limit('gemini'):\n",
        "#             logger.warning(\"Gemini rate limit reached\")\n",
        "#             return None\n",
        "\n",
        "#         try:\n",
        "#             model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "#             # Enhanced prompt for better JSON extraction\n",
        "#             enhanced_prompt = prompt + \"\"\"\n",
        "\n",
        "# IMPORTANT: Return ONLY valid JSON array. No explanations or markdown.\n",
        "# Example format:\n",
        "# [\n",
        "#   {\n",
        "#     \"name\": \"John Smith\",\n",
        "#     \"email\": \"john@example.com\",\n",
        "#     \"phone\": \"+1234567890\",\n",
        "#     \"confidence_score\": 0.95\n",
        "#   }\n",
        "# ]\n",
        "# \"\"\"\n",
        "\n",
        "#             response = model.generate_content(enhanced_prompt)\n",
        "#             self.request_history['gemini'].append(time.time())\n",
        "\n",
        "#             # Extract JSON from response\n",
        "#             response_text = response.text.strip()\n",
        "\n",
        "#             # Try to find JSON array in response\n",
        "#             json_match = re.search(r'\\[[\\s\\S]*\\]', response_text)\n",
        "#             if json_match:\n",
        "#                 contacts_data = json.loads(json_match.group())\n",
        "#                 return contacts_data\n",
        "\n",
        "#             return None\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Gemini API error: {e}\")\n",
        "#             return None\n",
        "\n",
        "#     @backoff.on_exception(\n",
        "#         backoff.expo,\n",
        "#         requests.exceptions.RequestException,\n",
        "#         max_tries=3\n",
        "#     )\n",
        "#     def _call_deepseek(self, prompt: str) -> Optional[List[Dict]]:\n",
        "#         \"\"\"Call DeepSeek API with retry logic\"\"\"\n",
        "#         if not self._check_rate_limit('deepseek'):\n",
        "#             logger.warning(\"DeepSeek rate limit reached\")\n",
        "#             return None\n",
        "\n",
        "#         try:\n",
        "#             headers = {\n",
        "#                 \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
        "#                 \"Content-Type\": \"application/json\"\n",
        "#             }\n",
        "\n",
        "#             data = {\n",
        "#                 \"model\": \"deepseek-chat\",\n",
        "#                 \"messages\": [{\n",
        "#                     \"role\": \"system\",\n",
        "#                     \"content\": \"You are a contact extraction expert. Always return valid JSON arrays only.\"\n",
        "#                 }, {\n",
        "#                     \"role\": \"user\",\n",
        "#                     \"content\": prompt\n",
        "#                 }],\n",
        "#                 \"temperature\": 0.1,\n",
        "#                 \"max_tokens\": 2000\n",
        "#             }\n",
        "\n",
        "#             response = requests.post(\n",
        "#                 \"https://api.deepseek.com/v1/chat/completions\",\n",
        "#                 headers=headers,\n",
        "#                 json=data,\n",
        "#                 timeout=30\n",
        "#             )\n",
        "\n",
        "#             if response.status_code == 200:\n",
        "#                 self.request_history['deepseek'].append(time.time())\n",
        "#                 result = response.json()\n",
        "#                 response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "#                 # Extract JSON\n",
        "#                 json_match = re.search(r'\\[[\\s\\S]*\\]', response_text)\n",
        "#                 if json_match:\n",
        "#                     return json.loads(json_match.group())\n",
        "\n",
        "#             return None\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"DeepSeek API error: {e}\")\n",
        "#             return None\n",
        "\n",
        "#     def extract_contacts_from_chunk(\n",
        "#         self,\n",
        "#         chunk: Dict[str, Any],\n",
        "#         doc_analysis: Dict[str, Any]\n",
        "#     ) -> List[Contact]:\n",
        "#         \"\"\"Extract contacts using available APIs with fallback\"\"\"\n",
        "\n",
        "#         # Generate optimal prompt\n",
        "#         prompt = self.prompt_engine.generate_prompt(chunk, doc_analysis)\n",
        "\n",
        "#         # Try APIs in priority order\n",
        "#         apis = sorted(\n",
        "#             [(name, cfg) for name, cfg in self.api_configs.items() if cfg['enabled']],\n",
        "#             key=lambda x: x[1]['priority']\n",
        "#         )\n",
        "\n",
        "#         for api_name, config in apis:\n",
        "#             logger.info(f\"Trying {api_name} API for extraction\")\n",
        "\n",
        "#             contacts_data = None\n",
        "#             if api_name == 'gemini':\n",
        "#                 contacts_data = self._call_gemini(prompt)\n",
        "#             elif api_name == 'deepseek':\n",
        "#                 contacts_data = self._call_deepseek(prompt)\n",
        "#             # Add more APIs as needed\n",
        "\n",
        "#             if contacts_data:\n",
        "#                 # Convert to Contact objects\n",
        "#                 contacts = []\n",
        "#                 for contact_dict in contacts_data:\n",
        "#                     if isinstance(contact_dict, dict):\n",
        "#                         contact = Contact(\n",
        "#                             name=contact_dict.get('name', '').strip(),\n",
        "#                             business_name=contact_dict.get('business_name', '').strip(),\n",
        "#                             email=contact_dict.get('email', '').strip(),\n",
        "#                             phone=contact_dict.get('phone', '').strip(),\n",
        "#                             address=contact_dict.get('address', '').strip(),\n",
        "#                             website=contact_dict.get('website', '').strip(),\n",
        "#                             title=contact_dict.get('title', '').strip(),\n",
        "#                             department=contact_dict.get('department', '').strip(),\n",
        "#                             linkedin=contact_dict.get('linkedin', '').strip(),\n",
        "#                             fax=contact_dict.get('fax', '').strip(),\n",
        "#                             mobile=contact_dict.get('mobile', '').strip(),\n",
        "#                             confidence_score=float(contact_dict.get('confidence_score', 0.7)),\n",
        "#                             source_context=chunk.get('content', '')[:200],\n",
        "#                             extraction_method=f\"{api_name}_{chunk.get('type', 'unknown')}\",\n",
        "#                             page_number=chunk.get('page', 0)\n",
        "#                         )\n",
        "#                         contacts.append(contact)\n",
        "\n",
        "#                 return contacts\n",
        "\n",
        "#         logger.warning(\"All API extractions failed\")\n",
        "#         return []\n",
        "\n",
        "# class AdvancedValidator:\n",
        "#     \"\"\"Multi-level validation with confidence scoring\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.email_validator = None\n",
        "#         try:\n",
        "#             import email_validator\n",
        "#             self.email_validator = email_validator\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "#     def validate_email(self, email: str) -> Tuple[bool, float]:\n",
        "#         \"\"\"Validate email with confidence score\"\"\"\n",
        "#         if not email:\n",
        "#             return False, 0.0\n",
        "\n",
        "#         # Basic pattern check\n",
        "#         basic_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "#         if not re.match(basic_pattern, email):\n",
        "#             return False, 0.0\n",
        "\n",
        "#         confidence = 0.7\n",
        "\n",
        "#         # Advanced validation if available\n",
        "#         if self.email_validator:\n",
        "#             try:\n",
        "#                 validation = self.email_validator.validate_email(email)\n",
        "#                 email = validation.email\n",
        "#                 confidence = 0.95\n",
        "#             except:\n",
        "#                 confidence = 0.5\n",
        "\n",
        "#         # Check for common patterns\n",
        "#         if any(domain in email.lower() for domain in ['gmail.', 'yahoo.', 'hotmail.', 'outlook.']):\n",
        "#             confidence = min(1.0, confidence + 0.1)\n",
        "\n",
        "#         # Check for suspicious patterns\n",
        "#         if email.count('@') > 1 or '..' in email:\n",
        "#             confidence *= 0.5\n",
        "\n",
        "#         return True, confidence\n",
        "\n",
        "#     def validate_phone(self, phone: str) -> Tuple[bool, float, str]:\n",
        "#         \"\"\"Validate and normalize phone with confidence\"\"\"\n",
        "#         if not phone:\n",
        "#             return False, 0.0, \"\"\n",
        "\n",
        "#         # Remove common formatting\n",
        "#         cleaned = re.sub(r'[^\\d+]', '', phone)\n",
        "\n",
        "#         if len(cleaned) < 7:\n",
        "#             return False, 0.0, \"\"\n",
        "\n",
        "#         confidence = 0.6\n",
        "#         normalized = cleaned\n",
        "\n",
        "#         try:\n",
        "#             # Try to parse with phonenumbers library\n",
        "#             import phonenumbers\n",
        "\n",
        "#             # Try with different country codes\n",
        "#             parsed = None\n",
        "#             for country in ['US', 'GB', 'PK', 'IN', None]:\n",
        "#                 try:\n",
        "#                     parsed = phonenumbers.parse(phone, country)\n",
        "#                     if phonenumbers.is_valid_number(parsed):\n",
        "#                         normalized = phonenumbers.format_number(\n",
        "#                             parsed,\n",
        "#                             phonenumbers.PhoneNumberFormat.INTERNATIONAL\n",
        "#                         )\n",
        "#                         confidence = 0.95\n",
        "#                         break\n",
        "#                 except:\n",
        "#                     continue\n",
        "\n",
        "#             if not parsed:\n",
        "#                 # Basic validation\n",
        "#                 if 10 <= len(cleaned) <= 15:\n",
        "#                     confidence = 0.7\n",
        "#                     normalized = cleaned\n",
        "#                 else:\n",
        "#                     confidence = 0.4\n",
        "\n",
        "#         except ImportError:\n",
        "#             # Fallback validation\n",
        "#             if re.match(r'^\\+?\\d{10,15}$', cleaned):\n",
        "#                 confidence = 0.7\n",
        "#                 normalized = cleaned\n",
        "\n",
        "#         return True, confidence, normalized\n",
        "\n",
        "#     def validate_website(self, url: str) -> Tuple[bool, float, str]:\n",
        "#         \"\"\"Validate and normalize website URL\"\"\"\n",
        "#         if not url:\n",
        "#             return False, 0.0, \"\"\n",
        "\n",
        "#         # Add protocol if missing\n",
        "#         if not url.startswith(('http://', 'https://')):\n",
        "#             url = 'https://' + url\n",
        "\n",
        "#         try:\n",
        "#             parsed = urlparse(url)\n",
        "#             if parsed.netloc:\n",
        "#                 # Check for common TLDs\n",
        "#                 tld = parsed.netloc.split('.')[-1].lower()\n",
        "#                 common_tlds = ['com', 'org', 'net', 'edu', 'gov', 'io', 'co', 'uk', 'de', 'fr']\n",
        "\n",
        "#                 confidence = 0.8 if tld in common_tlds else 0.6\n",
        "\n",
        "#                 # Check for suspicious patterns\n",
        "#                 if parsed.netloc.count('.') > 3:\n",
        "#                     confidence *= 0.7\n",
        "\n",
        "#                 return True, confidence, url\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "#         return False, 0.0, \"\"\n",
        "\n",
        "#     def validate_name(self, name: str) -> Tuple[bool, float]:\n",
        "#         \"\"\"Validate name with confidence\"\"\"\n",
        "#         if not name or len(name) < 2:\n",
        "#             return False, 0.0\n",
        "\n",
        "#         # Check for common patterns\n",
        "#         confidence = 0.7\n",
        "\n",
        "#         # Has first and last name\n",
        "#         if len(name.split()) >= 2:\n",
        "#             confidence += 0.1\n",
        "\n",
        "#         # Contains only valid characters\n",
        "#         if re.match(r'^[A-Za-z\\s\\'-\\.]+$', name):\n",
        "#             confidence += 0.1\n",
        "\n",
        "#         # Not too long\n",
        "#         if len(name) > 50:\n",
        "#             confidence *= 0.7\n",
        "\n",
        "#         # Has proper capitalization\n",
        "#         if name[0].isupper():\n",
        "#             confidence += 0.05\n",
        "\n",
        "#         return True, min(1.0, confidence)\n",
        "\n",
        "#     def calculate_overall_confidence(self, contact: Contact) -> float:\n",
        "#         \"\"\"Calculate overall confidence score for contact\"\"\"\n",
        "#         scores = []\n",
        "#         weights = []\n",
        "\n",
        "#         # Email validation\n",
        "#         if contact.email:\n",
        "#             valid, conf = self.validate_email(contact.email)\n",
        "#             if valid:\n",
        "#                 scores.append(conf)\n",
        "#                 weights.append(2.0)  # Email is important\n",
        "\n",
        "#         # Phone validation\n",
        "#         if contact.phone:\n",
        "#             valid, conf, _ = self.validate_phone(contact.phone)\n",
        "#             if valid:\n",
        "#                 scores.append(conf)\n",
        "#                 weights.append(2.0)  # Phone is important\n",
        "\n",
        "#         # Name validation\n",
        "#         if contact.name:\n",
        "#             valid, conf = self.validate_name(contact.name)\n",
        "#             if valid:\n",
        "#                 scores.append(conf)\n",
        "#                 weights.append(1.5)\n",
        "\n",
        "#         # Website validation\n",
        "#         if contact.website:\n",
        "#             valid, conf, _ = self.validate_website(contact.website)\n",
        "#             if valid:\n",
        "#                 scores.append(conf)\n",
        "#                 weights.append(1.0)\n",
        "\n",
        "#         # Business name\n",
        "#         if contact.business_name:\n",
        "#             scores.append(0.8)\n",
        "#             weights.append(1.0)\n",
        "\n",
        "#         # Must have email or phone\n",
        "#         if not contact.email and not contact.phone:\n",
        "#             return 0.0\n",
        "\n",
        "#         # Calculate weighted average\n",
        "#         if scores:\n",
        "#             weighted_sum = sum(s * w for s, w in zip(scores, weights))\n",
        "#             total_weight = sum(weights)\n",
        "#             base_confidence = weighted_sum / total_weight\n",
        "\n",
        "#             # Boost for completeness\n",
        "#             fields_filled = sum([\n",
        "#                 bool(contact.name),\n",
        "#                 bool(contact.email),\n",
        "#                 bool(contact.phone),\n",
        "#                 bool(contact.business_name),\n",
        "#                 bool(contact.address),\n",
        "#                 bool(contact.website),\n",
        "#                 bool(contact.title)\n",
        "#             ])\n",
        "\n",
        "#             completeness_boost = fields_filled / 7 * 0.2\n",
        "\n",
        "#             return min(1.0, base_confidence + completeness_boost)\n",
        "\n",
        "#         return contact.confidence_score\n",
        "\n",
        "# class IntelligentDeduplicator:\n",
        "#     \"\"\"Advanced deduplication using multiple similarity metrics\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.validators = AdvancedValidator()\n",
        "\n",
        "#     def calculate_similarity(self, contact1: Contact, contact2: Contact) -> float:\n",
        "#         \"\"\"Calculate similarity between two contacts\"\"\"\n",
        "#         scores = []\n",
        "#         weights = []\n",
        "\n",
        "#         # Email similarity (exact match only)\n",
        "#         if contact1.email and contact2.email:\n",
        "#             if contact1.email.lower() == contact2.email.lower():\n",
        "#                 scores.append(1.0)\n",
        "#                 weights.append(3.0)\n",
        "#             else:\n",
        "#                 scores.append(0.0)\n",
        "#                 weights.append(3.0)\n",
        "\n",
        "#         # Phone similarity\n",
        "#         if contact1.phone and contact2.phone:\n",
        "#             # Normalize phones\n",
        "#             _, _, norm1 = self.validators.validate_phone(contact1.phone)\n",
        "#             _, _, norm2 = self.validators.validate_phone(contact2.phone)\n",
        "\n",
        "#             if norm1 and norm2:\n",
        "#                 # Extract digits only\n",
        "#                 digits1 = re.sub(r'\\D', '', norm1)\n",
        "#                 digits2 = re.sub(r'\\D', '', norm2)\n",
        "\n",
        "#                 if digits1 == digits2:\n",
        "#                     scores.append(1.0)\n",
        "#                 elif digits1.endswith(digits2[-7:]) or digits2.endswith(digits1[-7:]):\n",
        "#                     scores.append(0.8)\n",
        "#                 else:\n",
        "#                     scores.append(0.0)\n",
        "#                 weights.append(2.5)\n",
        "\n",
        "#         # Name similarity\n",
        "#         if contact1.name and contact2.name:\n",
        "#             # Use fuzzy matching\n",
        "#             name_score = fuzz.token_sort_ratio(\n",
        "#                 contact1.name.lower(),\n",
        "#                 contact2.name.lower()\n",
        "#             ) / 100.0\n",
        "\n",
        "#             scores.append(name_score)\n",
        "#             weights.append(1.5)\n",
        "\n",
        "#         # Business name similarity\n",
        "#         if contact1.business_name and contact2.business_name:\n",
        "#             biz_score = fuzz.token_sort_ratio(\n",
        "#                 contact1.business_name.lower(),\n",
        "#                 contact2.business_name.lower()\n",
        "#             ) / 100.0\n",
        "\n",
        "#             scores.append(biz_score)\n",
        "#             weights.append(1.0)\n",
        "\n",
        "#         # Calculate weighted average\n",
        "#         if scores:\n",
        "#             weighted_sum = sum(s * w for s, w in zip(scores, weights))\n",
        "#             total_weight = sum(weights)\n",
        "#             return weighted_sum / total_weight\n",
        "\n",
        "#         return 0.0\n",
        "\n",
        "#     def merge_contacts(self, contact1: Contact, contact2: Contact) -> Contact:\n",
        "#         \"\"\"Intelligently merge two similar contacts\"\"\"\n",
        "#         # Use the contact with higher confidence as base\n",
        "#         if contact1.confidence_score >= contact2.confidence_score:\n",
        "#             merged = Contact(**contact1.__dict__)\n",
        "#             donor = contact2\n",
        "#         else:\n",
        "#             merged = Contact(**contact2.__dict__)\n",
        "#             donor = contact1\n",
        "\n",
        "#         # Fill in missing fields\n",
        "#         for field in ['name', 'business_name', 'email', 'phone', 'address',\n",
        "#                      'website', 'title', 'department', 'linkedin', 'fax', 'mobile']:\n",
        "#             if not getattr(merged, field) and getattr(donor, field):\n",
        "#                 setattr(merged, field, getattr(donor, field))\n",
        "\n",
        "#         # Merge metadata\n",
        "#         merged.source_context = f\"{merged.source_context}\\n{donor.source_context}\"\n",
        "#         merged.extraction_method = f\"{merged.extraction_method},{donor.extraction_method}\"\n",
        "\n",
        "#         # Recalculate confidence\n",
        "#         merged.confidence_score = max(\n",
        "#             merged.confidence_score,\n",
        "#             donor.confidence_score,\n",
        "#             (merged.confidence_score + donor.confidence_score) / 2 + 0.1\n",
        "#         )\n",
        "\n",
        "#         return merged\n",
        "\n",
        "#     def deduplicate(self, contacts: List[Contact], threshold: float = 0.7) -> List[Contact]:\n",
        "#         \"\"\"Deduplicate contacts using intelligent matching\"\"\"\n",
        "#         if not contacts:\n",
        "#             return []\n",
        "\n",
        "#         # Build similarity matrix\n",
        "#         n = len(contacts)\n",
        "#         similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "#         for i in range(n):\n",
        "#             for j in range(i + 1, n):\n",
        "#                 sim = self.calculate_similarity(contacts[i], contacts[j])\n",
        "#                 similarity_matrix[i][j] = sim\n",
        "#                 similarity_matrix[j][i] = sim\n",
        "\n",
        "#         # Find groups of similar contacts\n",
        "#         merged_indices = set()\n",
        "#         final_contacts = []\n",
        "\n",
        "#         for i in range(n):\n",
        "#             if i in merged_indices:\n",
        "#                 continue\n",
        "\n",
        "#             # Find all contacts similar to this one\n",
        "#             similar_indices = [i]\n",
        "#             for j in range(i + 1, n):\n",
        "#                 if j not in merged_indices and similarity_matrix[i][j] >= threshold:\n",
        "#                     similar_indices.append(j)\n",
        "#                     merged_indices.add(j)\n",
        "\n",
        "#             # Merge similar contacts\n",
        "#             if len(similar_indices) == 1:\n",
        "#                 final_contacts.append(contacts[i])\n",
        "#             else:\n",
        "#                 # Merge all similar contacts\n",
        "#                 merged = contacts[similar_indices[0]]\n",
        "#                 for idx in similar_indices[1:]:\n",
        "#                     merged = self.merge_contacts(merged, contacts[idx])\n",
        "#                 final_contacts.append(merged)\n",
        "\n",
        "#         # Sort by confidence score\n",
        "#         final_contacts.sort(key=lambda x: x.confidence_score, reverse=True)\n",
        "\n",
        "#         return final_contacts\n",
        "\n",
        "# class IntelligentContactExtractor:\n",
        "#     \"\"\"Main extraction pipeline with all components integrated\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.pdf_extractor = IntelligentPDFExtractor()\n",
        "#         self.api_manager = SmartAPIManager()\n",
        "#         self.validator = AdvancedValidator()\n",
        "#         self.deduplicator = IntelligentDeduplicator()\n",
        "\n",
        "#         # Load NLP model\n",
        "#         try:\n",
        "#             import spacy\n",
        "#             self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "#         except:\n",
        "#             logger.warning(\"spaCy model not loaded. Some features may be limited.\")\n",
        "#             self.nlp = None\n",
        "\n",
        "#         self.chunker = IntelligentTextChunker(self.nlp)\n",
        "\n",
        "#     def process_pdf(self, pdf_path: str, progress_callback=None) -> List[Contact]:\n",
        "#         \"\"\"Main extraction pipeline\"\"\"\n",
        "#         try:\n",
        "#             # Step 1: Extract content using multiple methods\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(0.1, \"Extracting PDF content...\")\n",
        "\n",
        "#             extraction_results = self.pdf_extractor.extract_all_methods(pdf_path)\n",
        "\n",
        "#             # Step 2: Analyze document structure\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(0.2, \"Analyzing document structure...\")\n",
        "\n",
        "#             doc_analysis = extraction_results.get('analysis', {})\n",
        "\n",
        "#             # Step 3: Create intelligent chunks\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(0.3, \"Creating intelligent chunks...\")\n",
        "\n",
        "#             chunks = self.chunker.chunk_by_structure(\n",
        "#                 extraction_results['text'],\n",
        "#                 extraction_results['tables']\n",
        "#             )\n",
        "\n",
        "#             # Step 4: Extract contacts from each chunk\n",
        "#             all_contacts = []\n",
        "#             total_chunks = len(chunks)\n",
        "\n",
        "#             for i, chunk in enumerate(chunks):\n",
        "#                 if progress_callback:\n",
        "#                     progress = 0.3 + (0.4 * (i / total_chunks))\n",
        "#                     progress_callback(progress, f\"Processing chunk {i+1}/{total_chunks}...\")\n",
        "\n",
        "#                 chunk_contacts = self.api_manager.extract_contacts_from_chunk(\n",
        "#                     chunk,\n",
        "#                     doc_analysis\n",
        "#                 )\n",
        "\n",
        "#                 # Validate and enhance each contact\n",
        "#                 for contact in chunk_contacts:\n",
        "#                     # Validate and normalize fields\n",
        "#                     if contact.email:\n",
        "#                         valid, conf = self.validator.validate_email(contact.email)\n",
        "#                         if not valid:\n",
        "#                             contact.email = \"\"\n",
        "\n",
        "#                     if contact.phone:\n",
        "#                         valid, conf, normalized = self.validator.validate_phone(contact.phone)\n",
        "#                         if valid:\n",
        "#                             contact.phone = normalized\n",
        "#                         else:\n",
        "#                             contact.phone = \"\"\n",
        "\n",
        "#                     if contact.website:\n",
        "#                         valid, conf, normalized = self.validator.validate_website(contact.website)\n",
        "#                         if valid:\n",
        "#                             contact.website = normalized\n",
        "\n",
        "#                     # Calculate overall confidence\n",
        "#                     contact.confidence_score = self.validator.calculate_overall_confidence(contact)\n",
        "\n",
        "#                     # Only keep valid contacts\n",
        "#                     if contact.confidence_score > 0.3 and (contact.email or contact.phone):\n",
        "#                         all_contacts.append(contact)\n",
        "\n",
        "#             # Step 5: Deduplicate contacts\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(0.8, \"Deduplicating contacts...\")\n",
        "\n",
        "#             unique_contacts = self.deduplicator.deduplicate(all_contacts)\n",
        "\n",
        "#             # Step 6: Final filtering\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(0.9, \"Finalizing results...\")\n",
        "\n",
        "#             final_contacts = [\n",
        "#                 contact for contact in unique_contacts\n",
        "#                 if contact.confidence_score >= 0.5\n",
        "#             ]\n",
        "\n",
        "#             if progress_callback:\n",
        "#                 progress_callback(1.0, f\"Extracted {len(final_contacts)} contacts!\")\n",
        "\n",
        "#             return final_contacts\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Extraction pipeline error: {e}\")\n",
        "#             logger.error(traceback.format_exc())\n",
        "#             raise\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Enhanced Streamlit application\"\"\"\n",
        "#     st.set_page_config(\n",
        "#         page_title=\"Intelligent Contact Extractor\",\n",
        "#         page_icon=\"🤖\",\n",
        "#         layout=\"wide\"\n",
        "#     )\n",
        "\n",
        "#     st.title(\"🤖 Intelligent Contact Extractor\")\n",
        "#     st.markdown(\"**AI-Powered Contact Extraction with Advanced Intelligence**\")\n",
        "\n",
        "#     # Initialize session state\n",
        "#     if 'extracted_contacts' not in st.session_state:\n",
        "#         st.session_state.extracted_contacts = []\n",
        "\n",
        "#     # Sidebar for configuration\n",
        "#     with st.sidebar:\n",
        "#         st.header(\"🔧 Configuration\")\n",
        "\n",
        "#         # API key inputs\n",
        "#         gemini_key = st.text_input(\n",
        "#             \"Gemini API Key\",\n",
        "#             value=GEMINI_API_KEY,\n",
        "#             type=\"password\",\n",
        "#             help=\"Get your free API key from Google AI Studio\"\n",
        "#         )\n",
        "#         if gemini_key:\n",
        "#             os.environ['GEMINI_API_KEY'] = gemini_key\n",
        "#             genai.configure(api_key=gemini_key)\n",
        "\n",
        "#         deepseek_key = st.text_input(\n",
        "#             \"DeepSeek API Key (Backup)\",\n",
        "#             value=DEEPSEEK_API_KEY,\n",
        "#             type=\"password\"\n",
        "#         )\n",
        "#         if deepseek_key:\n",
        "#             os.environ['DEEPSEEK_API_KEY'] = deepseek_key\n",
        "\n",
        "#         st.markdown(\"---\")\n",
        "\n",
        "#         # Settings\n",
        "#         st.subheader(\"Extraction Settings\")\n",
        "#         min_confidence = st.slider(\n",
        "#             \"Minimum Confidence Score\",\n",
        "#             0.0, 1.0, 0.5, 0.1,\n",
        "#             help=\"Contacts below this confidence will be filtered out\"\n",
        "#         )\n",
        "\n",
        "#         dedup_threshold = st.slider(\n",
        "#             \"Deduplication Threshold\",\n",
        "#             0.5, 1.0, 0.7, 0.05,\n",
        "#             help=\"Similarity threshold for merging duplicate contacts\"\n",
        "#         )\n",
        "\n",
        "#         st.markdown(\"---\")\n",
        "\n",
        "#         # API Status\n",
        "#         st.subheader(\"🔌 API Status\")\n",
        "\n",
        "#         api_status = {\n",
        "#             \"Gemini\": \"✅ Connected\" if gemini_key else \"❌ Not configured\",\n",
        "#             \"DeepSeek\": \"✅ Connected\" if deepseek_key else \"❌ Not configured\",\n",
        "#         }\n",
        "\n",
        "#         for api, status in api_status.items():\n",
        "#             st.write(f\"{api}: {status}\")\n",
        "\n",
        "#         st.markdown(\"---\")\n",
        "\n",
        "#         # Info\n",
        "#         st.info(\n",
        "#             \"This system uses advanced AI to intelligently extract contacts \"\n",
        "#             \"from any PDF format - tables, directories, business cards, or \"\n",
        "#             \"unstructured text.\"\n",
        "#         )\n",
        "\n",
        "#     # Main interface\n",
        "#     col1, col2 = st.columns([2, 1])\n",
        "\n",
        "#     with col1:\n",
        "#         st.header(\"📄 Upload Document\")\n",
        "#         uploaded_file = st.file_uploader(\n",
        "#             \"Upload PDF (Any format - we'll figure it out!)\",\n",
        "#             type=\"pdf\",\n",
        "#             help=\"Supports: Business directories, contact lists, tables, \"\n",
        "#                  \"business cards, membership rosters, and more\"\n",
        "#         )\n",
        "\n",
        "#     with col2:\n",
        "#         if uploaded_file:\n",
        "#             st.success(f\"✅ Uploaded: {uploaded_file.name}\")\n",
        "#             st.write(f\"Size: {uploaded_file.size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "#     if uploaded_file is not None:\n",
        "#         # Save uploaded file\n",
        "#         temp_path = f\"/tmp/{uploaded_file.name}\"\n",
        "#         with open(temp_path, \"wb\") as f:\n",
        "#             f.write(uploaded_file.getbuffer())\n",
        "\n",
        "#         # Extract contacts button\n",
        "#         if st.button(\"🚀 Extract Contacts Intelligently\", type=\"primary\"):\n",
        "#             # Create progress placeholder\n",
        "#             progress_bar = st.progress(0)\n",
        "#             status_text = st.empty()\n",
        "\n",
        "#             def update_progress(progress, message):\n",
        "#                 progress_bar.progress(progress)\n",
        "#                 status_text.text(message)\n",
        "\n",
        "#             try:\n",
        "#                 # Initialize extractor\n",
        "#                 extractor = IntelligentContactExtractor()\n",
        "\n",
        "#                 # Extract contacts\n",
        "#                 contacts = extractor.process_pdf(temp_path, update_progress)\n",
        "\n",
        "#                 # Store in session state\n",
        "#                 st.session_state.extracted_contacts = contacts\n",
        "\n",
        "#                 # Clear progress\n",
        "#                 progress_bar.empty()\n",
        "#                 status_text.empty()\n",
        "\n",
        "#                 if contacts:\n",
        "#                     st.success(f\"🎉 Successfully extracted {len(contacts)} high-quality contacts!\")\n",
        "#                 else:\n",
        "#                     st.warning(\"No contacts found. The document might not contain contact information.\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 st.error(f\"Error during extraction: {str(e)}\")\n",
        "#                 logger.error(traceback.format_exc())\n",
        "\n",
        "#         # Display results if available\n",
        "#         if st.session_state.extracted_contacts:\n",
        "#             contacts = st.session_state.extracted_contacts\n",
        "\n",
        "#             # Filter by confidence\n",
        "#             filtered_contacts = [\n",
        "#                 c for c in contacts\n",
        "#                 if c.confidence_score >= min_confidence\n",
        "#             ]\n",
        "\n",
        "#             st.header(\"📋 Extracted Contacts\")\n",
        "\n",
        "#             # Summary metrics\n",
        "#             col1, col2, col3, col4, col5 = st.columns(5)\n",
        "#             with col1:\n",
        "#                 st.metric(\"Total Contacts\", len(filtered_contacts))\n",
        "#             with col2:\n",
        "#                 avg_confidence = sum(c.confidence_score for c in filtered_contacts) / len(filtered_contacts)\n",
        "#                 st.metric(\"Avg Confidence\", f\"{avg_confidence:.2f}\")\n",
        "#             with col3:\n",
        "#                 complete_contacts = sum(1 for c in filtered_contacts if c.email and c.phone)\n",
        "#                 st.metric(\"Complete Contacts\", complete_contacts)\n",
        "#             with col4:\n",
        "#                 business_contacts = sum(1 for c in filtered_contacts if c.business_name)\n",
        "#                 st.metric(\"With Business Info\", business_contacts)\n",
        "#             with col5:\n",
        "#                 unique_methods = len(set(c.extraction_method for c in filtered_contacts))\n",
        "#                 st.metric(\"Extraction Methods\", unique_methods)\n",
        "\n",
        "#             # Display options\n",
        "#             display_format = st.radio(\n",
        "#                 \"Display Format\",\n",
        "#                 [\"Interactive Table\", \"Detailed Cards\", \"JSON View\"],\n",
        "#                 horizontal=True\n",
        "#             )\n",
        "\n",
        "#             if display_format == \"Interactive Table\":\n",
        "#                 # Convert to DataFrame for display\n",
        "#                 df_data = []\n",
        "#                 for contact in filtered_contacts:\n",
        "#                     df_data.append({\n",
        "#                         \"Name\": contact.name,\n",
        "#                         \"Business\": contact.business_name,\n",
        "#                         \"Email\": contact.email,\n",
        "#                         \"Phone\": contact.phone,\n",
        "#                         \"Title\": contact.title,\n",
        "#                         \"Website\": contact.website,\n",
        "#                         \"Confidence\": f\"{contact.confidence_score:.2f}\",\n",
        "#                         \"Method\": contact.extraction_method.split('_')[0]\n",
        "#                     })\n",
        "\n",
        "#                 df = pd.DataFrame(df_data)\n",
        "\n",
        "#                 # Editable dataframe\n",
        "#                 edited_df = st.data_editor(\n",
        "#                     df,\n",
        "#                     use_container_width=True,\n",
        "#                     num_rows=\"dynamic\",\n",
        "#                     column_config={\n",
        "#                         \"Confidence\": st.column_config.ProgressColumn(\n",
        "#                             \"Confidence\",\n",
        "#                             help=\"AI confidence in this contact\",\n",
        "#                             format=\"%.2f\",\n",
        "#                             min_value=0,\n",
        "#                             max_value=1,\n",
        "#                         ),\n",
        "#                     }\n",
        "#                 )\n",
        "\n",
        "#             elif display_format == \"Detailed Cards\":\n",
        "#                 # Display as expandable cards\n",
        "#                 for i, contact in enumerate(filtered_contacts):\n",
        "#                     with st.expander(\n",
        "#                         f\"📇 {contact.name or 'Contact ' + str(i+1)} \"\n",
        "#                         f\"(Confidence: {contact.confidence_score:.2f})\"\n",
        "#                     ):\n",
        "#                         col1, col2 = st.columns(2)\n",
        "\n",
        "#                         with col1:\n",
        "#                             st.write(\"**Personal Information**\")\n",
        "#                             st.write(f\"Name: {contact.name}\")\n",
        "#                             st.write(f\"Title: {contact.title}\")\n",
        "#                             st.write(f\"Email: {contact.email}\")\n",
        "#                             st.write(f\"Phone: {contact.phone}\")\n",
        "#                             if contact.mobile:\n",
        "#                                 st.write(f\"Mobile: {contact.mobile}\")\n",
        "#                             if contact.fax:\n",
        "#                                 st.write(f\"Fax: {contact.fax}\")\n",
        "\n",
        "#                         with col2:\n",
        "#                             st.write(\"**Business Information**\")\n",
        "#                             st.write(f\"Company: {contact.business_name}\")\n",
        "#                             st.write(f\"Department: {contact.department}\")\n",
        "#                             st.write(f\"Website: {contact.website}\")\n",
        "#                             st.write(f\"Address: {contact.address}\")\n",
        "#                             if contact.linkedin:\n",
        "#                                 st.write(f\"LinkedIn: {contact.linkedin}\")\n",
        "\n",
        "#                         st.write(\"**Metadata**\")\n",
        "#                         st.write(f\"Extraction Method: {contact.extraction_method}\")\n",
        "#                         st.write(f\"Page: {contact.page_number}\")\n",
        "\n",
        "#                         if contact.source_context:\n",
        "#                             st.text_area(\n",
        "#                                 \"Source Context\",\n",
        "#                                 value=contact.source_context,\n",
        "#                                 height=50,\n",
        "#                                 disabled=True\n",
        "#                             )\n",
        "\n",
        "#             else:  # JSON View\n",
        "#                 # Display as JSON\n",
        "#                 json_data = [contact.to_dict() for contact in filtered_contacts]\n",
        "#                 st.json(json_data)\n",
        "\n",
        "#             # Export options\n",
        "#             st.header(\"💾 Export Data\")\n",
        "\n",
        "#             col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "#             with col1:\n",
        "#                 # CSV export\n",
        "#                 csv_data = pd.DataFrame([c.to_dict() for c in filtered_contacts]).to_csv(index=False)\n",
        "#                 st.download_button(\n",
        "#                     label=\"📊 Download CSV\",\n",
        "#                     data=csv_data,\n",
        "#                     file_name=f\"contacts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
        "#                     mime=\"text/csv\"\n",
        "#                 )\n",
        "\n",
        "#             with col2:\n",
        "#                 # JSON export\n",
        "#                 json_data = json.dumps([c.to_dict() for c in filtered_contacts], indent=2)\n",
        "#                 st.download_button(\n",
        "#                     label=\"📥 Download JSON\",\n",
        "#                     data=json_data,\n",
        "#                     file_name=f\"contacts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
        "#                     mime=\"application/json\"\n",
        "#                 )\n",
        "\n",
        "#             with col3:\n",
        "#                 # Excel export\n",
        "#                 df = pd.DataFrame([c.to_dict() for c in filtered_contacts])\n",
        "#                 excel_buffer = pd.ExcelWriter('buffer.xlsx', engine='xlsxwriter')\n",
        "#                 df.to_excel(excel_buffer, index=False)\n",
        "#                 excel_data = excel_buffer.to_bytes()\n",
        "\n",
        "#                 st.download_button(\n",
        "#                     label=\"📑 Download Excel\",\n",
        "#                     data=excel_data,\n",
        "#                     file_name=f\"contacts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\",\n",
        "#                     mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "#                 )\n",
        "\n",
        "#             with col4:\n",
        "#                 # Email list export\n",
        "#                 email_list = [c.email for c in filtered_contacts if c.email]\n",
        "#                 email_data = \"\\n\".join(email_list)\n",
        "\n",
        "#                 st.download_button(\n",
        "#                     label=\"📧 Email List\",\n",
        "#                     data=email_data,\n",
        "#                     file_name=f\"emails_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\",\n",
        "#                     mime=\"text/plain\"\n",
        "#                 )\n",
        "\n",
        "#         # Clean up temp file\n",
        "#         if os.path.exists(temp_path):\n",
        "#             os.remove(temp_path)\n",
        "\n",
        "#     # Instructions\n",
        "#     with st.expander(\"📖 How This Intelligent System Works\"):\n",
        "#         st.markdown(\"\"\"\n",
        "#         ### 🧠 Advanced Intelligence Features\n",
        "\n",
        "#         **1. Multi-Method PDF Extraction**\n",
        "#         - Uses 5+ different extraction methods (PDFPlumber, Camelot, Tabula, PDFMiner, PyMuPDF, OCR)\n",
        "#         - Automatically selects best method based on document analysis\n",
        "#         - Handles tables, scanned documents, and complex layouts\n",
        "\n",
        "#         **2. Document Understanding**\n",
        "#         - Analyzes document structure and type (table, directory, business cards, etc.)\n",
        "#         - Adapts extraction strategy based on document characteristics\n",
        "#         - Preserves context and relationships between data\n",
        "\n",
        "#         **3. Intelligent Chunking**\n",
        "#         - Context-aware chunking that doesn't break contact information\n",
        "#         - Semantic understanding of text boundaries\n",
        "#         - Table-aware processing\n",
        "\n",
        "#         **4. Adaptive AI Prompting**\n",
        "#         - Generates specialized prompts based on document type\n",
        "#         - Uses document metadata to improve extraction accuracy\n",
        "#         - Multi-API support with intelligent fallback\n",
        "\n",
        "#         **5. Advanced Validation**\n",
        "#         - Multi-level validation for emails, phones, websites\n",
        "#         - International phone number support\n",
        "#         - Confidence scoring based on data quality\n",
        "\n",
        "#         **6. Smart Deduplication**\n",
        "#         - ML-based similarity matching\n",
        "#         - Intelligent contact merging\n",
        "#         - Preserves best information from duplicates\n",
        "\n",
        "#         ### 📄 Supported Document Types\n",
        "#         - ✅ Business directories\n",
        "#         - ✅ Membership lists\n",
        "#         - ✅ Contact tables\n",
        "#         - ✅ Business cards (scanned or digital)\n",
        "#         - ✅ Company rosters\n",
        "#         - ✅ Trade show attendee lists\n",
        "#         - ✅ Professional associations\n",
        "#         - ✅ Unstructured documents with embedded contacts\n",
        "\n",
        "#         ### 🔧 Pro Tips\n",
        "#         - The system automatically handles different formats - no configuration needed\n",
        "#         - Higher resolution PDFs yield better results for scanned documents\n",
        "#         - Confidence scores help identify the most reliable contacts\n",
        "#         - Export in multiple formats for different use cases\n",
        "#         \"\"\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "# '''\n",
        "\n",
        "# with open('contact_extractor.py', 'w') as f:\n",
        "#     f.write(script_content)\n",
        "\n",
        "# print(\"✅ Script created successfully!\")"
      ],
      "metadata": {
        "id": "6KW_puVxkn09"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS32bC-2v-MF",
        "outputId": "7fe3cad5-7148-4d11-db7a-7dd34324e093"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "def run_streamlit():\n",
        "    subprocess.run([\n",
        "        \"streamlit\", \"run\",\n",
        "        \"/content/drive/MyDrive/Leads_Extraction_Data_UKPakTrade/enhanced_contact_extractor.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.headless\", \"true\"\n",
        "    ])\n",
        "\n",
        "# Start Streamlit in background\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"🌐 Access your app at: {public_url}\")\n",
        "print(f\"📱 Click the link above to open the Contact Extractor interface\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu9BboPrkq9b",
        "outputId": "e87d9e2a-73f2-4e81-8e34-90274fa07bd3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Access your app at: NgrokTunnel: \"https://5b8dffdf1de8.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "📱 Click the link above to open the Contact Extractor interface\n"
          ]
        }
      ]
    }
  ]
}